{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ef9efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:05:50 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf888ee018b48dd949a23609eafd89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 20:05:50 INFO: Downloaded file to C:\\Users\\huvif\\stanza_resources\\resources.json\n",
      "2025-08-25 20:05:50 WARNING: Language fi package default expects mwt, which has been added\n",
      "2025-08-25 20:05:50 INFO: Loading these models for language: fi (Finnish):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | tdt          |\n",
      "| mwt       | tdt          |\n",
      "| pos       | tdt_charlm   |\n",
      "| lemma     | tdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-08-25 20:05:50 INFO: Using device: cpu\n",
      "2025-08-25 20:05:50 INFO: Loading: tokenize\n",
      "2025-08-25 20:05:51 INFO: Loading: mwt\n",
      "2025-08-25 20:05:51 INFO: Loading: pos\n",
      "2025-08-25 20:05:51 INFO: Loading: lemma\n",
      "2025-08-25 20:05:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alkuperäinen stop-sanojen määrä: 847\n",
      "Karsittu stop-sanojen määrä: 841\n",
      "Esimerkkejä: ['aikovat', 'aina', 'ainakaan', 'ainakin', 'ainoa', 'ainoat', 'aiomme', 'aion', 'aiotte', 'aist']\n",
      "       otsikko                                             teksti   tahdet  \\\n",
      "0  placeholder  Ostin hyvästä tarjouksesta pesukoneen asennuks...  5.touko   \n",
      "1  placeholder  Ensimmäiset viikot pyykätty ja tuntuisi olevan...  4.touko   \n",
      "2  placeholder  Onhan näissä koneissa eroa kun ostaa 300€ kone...  5.touko   \n",
      "3  placeholder  Vasta kaksi kertaa käytetty: mix -ohjelma ja l...  4.touko   \n",
      "4  placeholder  Hommattiin pari viikkoa sitten ja oikeastaan s...  5.touko   \n",
      "\n",
      "                                                 url         tunne  \n",
      "0  https://www.gigantti.fi/product/kodinkoneet/py...  positiivinen  \n",
      "1  https://www.gigantti.fi/product/kodinkoneet/py...  positiivinen  \n",
      "2  https://www.gigantti.fi/product/kodinkoneet/py...  positiivinen  \n",
      "3  https://www.gigantti.fi/product/kodinkoneet/py...  positiivinen  \n",
      "4  https://www.gigantti.fi/product/kodinkoneet/py...  positiivinen  \n",
      "Accuracy: 0.7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "negatiivinen       0.62      0.83      0.71         6\n",
      "   neutraali       0.00      0.00      0.00         3\n",
      "positiivinen       0.82      0.82      0.82        11\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.48      0.55      0.51        20\n",
      "weighted avg       0.64      0.70      0.66        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#Stanza pipeline suomen kieltä varten\n",
    "nlp = stanza.Pipeline('fi', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "\n",
    "#Suomenkielisten stop-sanojen lista\n",
    "url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-fi/master/stopwords-fi.txt\"\n",
    "response = requests.get(url)\n",
    "#Muodostetaan stop sanoista lista, jossa jokainen sana on oma alkionsa\n",
    "finnish_stop_words = response.text.splitlines()\n",
    "suodatus = {'ei', 'eikä', 'mutta', 'vaan', 'vaikka', 'jos', 'kuin'} #näitä ei poisteta\n",
    "custom_stop_words = [word for word in finnish_stop_words if word not in suodatus]\n",
    "custom_stop_words.append('placeholder') #tyhjien otsikoiden tilalle laitettu teksti\n",
    "print(f\"Alkuperäinen stop-sanojen määrä: {len(finnish_stop_words)}\")\n",
    "print(f\"Karsittu stop-sanojen määrä: {len(custom_stop_words)}\")\n",
    "print(\"Esimerkkejä:\", custom_stop_words[10:20])\n",
    "\n",
    "#Funktio tekstin lemmatisointia varten\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Aineisto\n",
    "df = pd.read_csv('arvostelut.csv', delimiter=';', quoting=0)\n",
    "print(df.head())\n",
    "#Lemmatisoinnin sovellus\n",
    "# Syöte X, data jonka avulla malli oppii. Kohde y, se mitä mallin tulee oppia ennustamaan.\n",
    "df['lemmatized_text'] = df['teksti'].apply(lemmatize_text)\n",
    "df['lemmatized_title'] = df['otsikko'].apply(lemmatize_text)\n",
    "X = df['lemmatized_title'] + df['lemmatized_text']\n",
    "#X = df['teksti']\n",
    "y = df['tunne']\n",
    "\n",
    "# train_test_split sekoittaa datan ja jakaa sen neljään osaan: X_train ja y_train opetusdata, X_test ja y_test testidata\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=77, stratify=y\n",
    ")\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "from sklearn.pipeline import Pipeline #ketjuttaa datan muokkausvaiheet ja malli yhteen pakettiin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=custom_stop_words)), #vastaanottaa tekstin ja muuttaa sen numeroiksi TF-IDF:n avulla\n",
    "    ('clf', LogisticRegression(class_weight='balanced')), #Luokittelija, ottaa vastaan TfidVectorizer:n tuottaman numerodatan ja tekee ennusteita sen perusteella. \n",
    "    #('clf', MultinomialNB()),\n",
    "    #('clf', LinearSVC(class_weight='balanced', dual=\"auto\")),\n",
    "])\n",
    "\n",
    "\n",
    "text_clf.fit(X_train, y_train) #.fit() komento syöttää opetusdatan X_train ja y_train putken läpi, TfidVectorizer analysoi X_trainin sanat ja luo sanaston sekä laskee TF-IDF arvot.\n",
    "\n",
    "#Numerodata ja vastaukset y_train syötetään mallille joka oppii mitkä sanat liittyvät mihinkin tunteeseen\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "predictions = text_clf.predict(X_test) #Ennusteet testidatalla\n",
    "#Tulostetaan raportti\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
